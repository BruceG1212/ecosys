CREATE QUERY training_hybridModel(DOUBLE alpha = 0.001, DOUBLE lambda = 0.00005, INT Iter=100, INT num_latent_factors = 19) FOR GRAPH Recommender {
	ListAccum<DOUBLE> @tmp;
	ArrayAccum<SumAccum<double>> @theta[19];  #modify list length according to num_latent_factors
	ArrayAccum<SumAccum<double>> @x[19];
	ArrayAccum<SumAccum<double>> @Gradient[19];
	SumAccum<DOUBLE> @@RMSE_training;
	SumAccum<DOUBLE> @@RMSE_validation;
	AndAccum @label;
#	DOUBLE cnt_training = 70737;  #ml-20m:  14189115
#  DOUBLE cnt_validation = 29274;  #ml-20m: 5811148
  DOUBLE cnt_training = 69767;  #ml-100k:  100000-30233
  DOUBLE cnt_validation = 30233;  #ml-100k: 30233
#	DOUBLE cnt_training =   14189115;
#  DOUBLE cnt_validation =  5811148;



#	DOUBLE lambda = 10;
	## pass x and theta to local accum


	MOVIEs = {MOVIE.*};


	MOVIEs = SELECT s FROM MOVIEs:s
	         ACCUM
	           FOREACH i IN RANGE[0,num_latent_factors-1] DO
	             s.@x[i] += s.x.get(i)
	           END;

	USERs = {USER.*};
	USERs = SELECT s FROM USERs:s
	        ACCUM
	           FOREACH i IN RANGE[0,num_latent_factors-1] DO
	             s.@theta[i] += s.theta.get(i)
	           END;



	## compute gradient
	WHILE TRUE LIMIT Iter DO
	@@RMSE_training = 0;
	@@RMSE_validation = 0;
	USERs = SELECT s FROM USERs:s
	        ACCUM
	          s.@Gradient.reallocate(num_latent_factors);

  MOVIEs = SELECT s FROM MOVIEs:s
	        ACCUM
	          s.@Gradient.reallocate(num_latent_factors);

	USERs = SELECT s FROM USERs:s -(rate:e)-> MOVIE:t
	        ACCUM
	          DOUBLE delta = dotProduct_ArrayAccum_ArrayAccum(s.@theta,t.@x),
	          delta = delta+e.TFIDF_prediction-e.rating,
	          IF e.label THEN
	            @@RMSE_training += delta*delta,
	            s.@Gradient += product_ArrayAccum_const(t.@x,delta),
	            t.@Gradient += product_ArrayAccum_const(s.@theta,delta)
	          ELSE
	            @@RMSE_validation += delta*delta
	          END
	        POST-ACCUM
#	          @@cost_training += 0.5*dotProduct_ArrayAccum_ArrayAccum(s.@theta, s.@theta),
	          s.@Gradient += product_ArrayAccum_const(s.@theta,lambda),
	          s.@theta += product_ArrayAccum_const(s.@Gradient,-alpha),
#	          @@cost_training += 0.5*dotProduct_ArrayAccum_ArrayAccum(t.@x, t.@x),
	          t.@Gradient += product_ArrayAccum_const(t.@x,lambda),
	          t.@x += product_ArrayAccum_const(t.@Gradient,-alpha);


	@@RMSE_training = sqrt(@@RMSE_training/cnt_training);
	PRINT @@RMSE_training;
	@@RMSE_validation = sqrt(@@RMSE_validation/cnt_validation);
	PRINT @@RMSE_validation;

	END;


	## pass local accum to x and theta
	MOVIEs = SELECT s FROM MOVIEs:s
	         POST-ACCUM
	           s.@tmp.clear(),
	           FOREACH i IN RANGE[0,num_latent_factors-1] DO
#	             s.x.update(i,s.@x[i])
	             s.@tmp += s.@x[i]
	           END,
	           s.x = s.@tmp;


	USERs = SELECT s FROM USERs:s
	        POST-ACCUM
	           s.@tmp.clear(),
	           FOREACH i IN RANGE[0,num_latent_factors-1] DO
#	             s.x.update(i,s.@x[i])
	             s.@tmp += s.@theta[i]
	           END,
	           s.theta = s.@tmp;

#  PRINT M1;
}

