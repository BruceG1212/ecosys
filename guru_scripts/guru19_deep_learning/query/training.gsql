CREATE QUERY training(DOUBLE alpha = 1.0) FOR GRAPH NeuralNetwork {
  ArrayAccum<SumAccum<DOUBLE>> @a_training[4000];  # number of training set m = 4000
	ArrayAccum<SumAccum<DOUBLE>> @delta_training[4000];
	ArrayAccum<SumAccum<DOUBLE>> @a_validation[1000];  # number of validation set: 1000
	ArrayAccum<SumAccum<DOUBLE>> @delta_validation[1000];

	SumAccum<INT> @@iter;
	SumAccum<DOUBLE> @@cost_training;
	SumAccum<DOUBLE> @@cost_validation;

	ListAccum<DOUBLE> @@ones_training;  #  vector with all entries equal to one
	ListAccum<DOUBLE> @@ones_validation;


	DOUBLE lambda = 3;  # regularization parameter
	# DOUBLE alpha = 1;  #learning rate

	@@ones_training += unit_List(4000);  # initialize the vector with all entries equal to one
	@@ones_validation += unit_List(1000);

	InputLayer = {inputLayer.*};
	HiddenLayer = {hiddenLayer.*};
	OutputLayer = {outputLayer.*};



	/* forward propogate a */
	  @@iter += 1;
	  @@cost_training = 0;
	  @@cost_validation = 0;
	  HiddenLayer = SELECT s FROM HiddenLayer:s
	                ACCUM
	                  s.@a_training.reallocate(4000),
	                  s.@delta_training.reallocate(4000),
	                  s.@a_validation.reallocate(1000),
	                  s.@delta_validation.reallocate(1000);


	  OutputLayer = SELECT s FROM OutputLayer:s
	                ACCUM
	                  s.@a_training.reallocate(4000),
	                  s.@delta_training.reallocate(4000),
	                  s.@a_validation.reallocate(1000),
	                  s.@delta_validation.reallocate(1000);


	  InputLayer = SELECT s FROM InputLayer:s -(Theta1:e)->hiddenLayer:t
	               ACCUM
	                 IF s.v_in_id == "401" THEN
	                     t.@a_training += product_List_const(@@ones_training, e.theta),
                         # FOREACH i IN RANGE[0, 3999] DO
                         #  t.@a_training[i] += e.theta
                         # END
	                     t.@a_validation += product_List_const(@@ones_validation, e.theta)
                         # FOREACH i IN RANGE[0, 999] DO
                         #  t.@a_validation[i] += e.theta
                         # END
	                 ELSE
	                     t.@a_training += product_List_const(s.x_training, e.theta),
                         # FOREACH i IN RANGE[0, 3999] DO
                         #  t.@a_training[i] += s.x_training.get(i)*e.theta
                         # END
	                     t.@a_validation += product_List_const(s.x_validation, e.theta)
                         # FOREACH i IN RANGE[0, 999] DO
                         #  t.@a_validation[i] += s.x_validation.get(i)*e.theta
                         # END
	                 END
	               POST-ACCUM
	                 t.@a_training += sigmoid_ArrayAccum(t.@a_training),
                        # FOREACH i IN RANGE[0, 3999] DO
                        #  t.@a_training[i] += 1/(1+exp(-t.@a_training[i]))-t.@a_training[i]
                        #  END;
	                 t.@a_validation += sigmoid_ArrayAccum(t.@a_validation);
                        # FOREACH i IN RANGE[0, 999] DO
                        #  t.@a_validation[i] += 1/(1+exp(-t.@a_validation[i]))-t.@a_validation[i]
                        #  END;

	  HiddenLayer = SELECT s FROM HiddenLayer:s -(Theta2:e)->outputLayer:t
	                ACCUM
	                  IF s.v_hid_id == "26" THEN
	                    t.@a_training += product_List_const(@@ones_training, e.theta),
                         # FOREACH i IN RANGE[0, 3999] DO
                         #  t.@a_training[i] += e.theta
                         # END
	                    t.@a_validation += product_List_const(@@ones_validation, e.theta)
                         # FOREACH i IN RANGE[0, 999] DO
                         #  t.@a_validation[i] += e.theta
                         # END
	                  ELSE
	                    t.@a_training += product_ArrayAccum_const(s.@a_training, e.theta),
                         # FOREACH i IN RANGE[0, 3999] DO
                         #  t.@a_training[i] += s.@a_training[i]*e.theta
                         # END
	                    t.@a_validation += product_ArrayAccum_const(s.@a_validation, e.theta)
                         # FOREACH i IN RANGE[0, 999] DO
                         #  t.@a_validation[i] += s.@a_validation[i]*e.theta
                         # END
	                  END
	                POST-ACCUM
	                  t.@a_training += sigmoid_ArrayAccum(t.@a_training),
                        # FOREACH i IN RANGE[0, 3999] DO
                        #  t.@a_training[i] += 1/(1+exp(-t.@a_training[i]))-t.@a_training[i]
                        #  END
	                  t.@a_validation += sigmoid_ArrayAccum(t.@a_validation);
                        # FOREACH i IN RANGE[0, 999] DO
                        #  t.@a_validation[i] += 1/(1+exp(-t.@a_validation[i]))-t.@a_validation[i]
                        #  END;
#	PRINT OutputLayer.@a[0];


	/* back propogate delta */

	  OutputLayer = SELECT s FROM OutputLayer:s
	                ACCUM
	                  s.@delta_training += diff_ArrayAccum_List(s.@a_training, s.y_training),
	                  s.@delta_validation += diff_ArrayAccum_List(s.@a_validation, s.y_validation),
	                  @@cost_training += cost_ArrayAccum_List(s.@a_training, s.y_training),
	                  @@cost_validation += cost_ArrayAccum_List(s.@a_validation, s.y_validation);
                        # FOREACH i IN RANGE[0, 3999] DO
                        # s.@delta_training[i] += s.@a_training[i]-s.y_training.get(i),
                        # @@cost += -(s.y_training.get(i)*log(s.@a_training[i])+(1-s.y_training.get(i))*log(1-s.@a_training[i]))
                        # END;
                        # FOREACH i IN RANGE[0, 999] DO
                        # s.@delta_validation[i] += s.@delta_validation[i]-s.y_validation.get(i),
                        # @@cost += -(s.y_validation.get(i)*log(s.@a_validation[i])+(1-s.y_validation.get(i))*log(1-s.@a_validation[i]))
                        # END;

	  OutputLayer = SELECT s FROM OutputLayer:s -(Theta2:e)->hiddenLayer:t
	                ACCUM
	                  t.@delta_training += product_ArrayAccum_const(s.@delta_training, e.theta)
                         # FOREACH i IN RANGE[0, 3999] DO
                         #  t.@delta_training[i] += s.@delta_training[i]*e.theta
                         # END
	                POST-ACCUM
	                  t.@delta_training += delta_ArrayAccum(t.@delta_training, t.@a_training);
                        # FOREACH i IN RANGE[0, 3999] DO
                        #   t.@delta_training[i] += t.@delta_training[i]*t.@a_training[i]*(1-t.@a_training[i])-t.@delta_training[i]
                        # END;


	/* update theta */
	  InputLayer = {inputLayer.*};
	  InputLayer = SELECT s FROM InputLayer:s -(Theta1:e)->hiddenLayer:t
	               ACCUM
	                 @@cost_training += lambda/2*e.theta*e.theta,
	                 @@cost_validation += lambda/2*e.theta*e.theta,
	                 DOUBLE D = 0,

                        # FOREACH i IN RANGE[0, 3999] DO
                        #   D = D + s.s.x_training.get(i)*t.@delta_training[i]
                        # END,
	                 IF s.v_in_id == "401" THEN
	                    D = dotProduct_ArrayAccum_List(t.@delta_training, @@ones_training),
                      D = D/4000
	                 ELSE
	                   D = dotProduct_ArrayAccum_List(t.@delta_training, s.x_training),
	                   D = D/4000+lambda*e.theta/4000
	                 END,
	                 e.theta = e.theta - alpha * D;

	  HiddenLayer = {hiddenLayer.*};
	  HiddenLayer = SELECT s FROM HiddenLayer:s -(Theta2:e)->outputLayer:t
	                ACCUM
	                  @@cost_training += lambda/2*e.theta*e.theta,
	                  @@cost_validation += lambda/2*e.theta*e.theta,
	                  DOUBLE D = 0,
                        # FOREACH i IN RANGE[0, 3999] DO
                        #   D = D + s.a_training[i]*t.@delta_training[i]
                        # END,
	                  IF s.v_hid_id == "26" THEN
	                    D = dotProduct_ArrayAccum_List(t.@delta_training, @@ones_training),
                      D = D/4000
	                  ELSE
	                    D = dotProduct_ArrayAccum_ArrayAccum(s.@a_training, t.@delta_training),
	                    D = D/4000+lambda*e.theta/4000
	                  END,
	                  e.theta = e.theta - alpha * D;

  @@cost_training = @@cost_training/4000;
	@@cost_validation = @@cost_validation/1000;
	PRINT @@cost_training;
	PRINT @@cost_validation;

}
