package graphsql;

import java.io.Serializable;
import java.io.BufferedReader;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.FileInputStream;
import java.io.OutputStreamWriter;
import java.io.File;
import java.io.IOException;
import java.lang.StringBuffer;
import java.lang.StringBuilder;
import java.net.URL;
import java.net.HttpURLConnection;
import java.util.Random;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.ListIterator;
import java.util.Properties;
import scala.collection.JavaConversions;
import scala.collection.Seq;
import java.util.Objects;
//import java.sql.Timestamp;

import org.apache.spark.sql.Dataset;
//import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.types.*;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Encoder;
import org.apache.spark.sql.Encoders;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.MapFunction;

import com.amazonaws.ClientConfiguration;
import com.amazonaws.services.s3.*;
import com.amazonaws.services.s3.model.*;
import com.amazonaws.services.s3.iterable.*;
import com.amazonaws.auth.BasicAWSCredentials;

public class generateDataset implements Serializable {

  static String S3Prefix = "s3a://";
  static String S3FileSeparator = "/";
  static String localPrefix = "file://";

  static String awsAccessKey;
  static String awsSecretKey;
  static String bucket_name;
  static String[] schema_folders;

  public static void main(String[] args) {
    String localStorage = "";
    String properties_file = "";

    if (args.length < 1) {
      System.out.println("No arguments received, exit");
      System.exit(1);
    } else {
      properties_file = args[0];
      localStorage = localPrefix + args[1];
    }

    try {
      // read from properties file
      Properties prop = new Properties();
      prop.load(new FileInputStream(properties_file));

      awsAccessKey = prop.getProperty("awsAccessKey");
      awsSecretKey = prop.getProperty("awsSecretKey");
      bucket_name = prop.getProperty("bucket_name");
      schema_folders = prop.getProperty("schema_folders").split(",");

      new generateDataset().run(localStorage);
    } catch (IOException ex) {
      ex.printStackTrace();
    }
  }


  public void run(String localStorage) {
    // set the spark config
    SparkConf conf = new SparkConf().setAppName("uber");
    conf.set("fs.s3a.access.key", awsAccessKey);
    conf.set("fs.s3a.secret.key", awsSecretKey);
    conf.set("fs.s3a.attempts.maximum", "30");
    conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem");
    // conf.set("mapreduce.fileoutputcommitter.algorithm.version", "2")

    // create spark context and spark sql instance
    JavaSparkContext context = new JavaSparkContext(conf);
    SQLContext sc = new SQLContext(context);

    // set the S3 client configuration and create a client connected to S3.
    ClientConfiguration configuration = new ClientConfiguration();
    configuration.setMaxErrorRetry(10);
    configuration.setConnectionTimeout(501000);
    configuration.setSocketTimeout(501000);
    //configuration.setUseTcpKeepAlive(true);
    AmazonS3Client s3client = new AmazonS3Client(new BasicAWSCredentials(awsAccessKey, awsSecretKey), configuration);

    System.out.println("------start process---");
    for (String schema_folder : schema_folders) {
      transfer2CSV(s3client, sc, schema_folder, localStorage);
    }
    System.out.println("------end process---");
  }


  /*
    transfer parquet file to csv.
    param:  1. s3client: s3 client used to read from S3
            2. sc: spark sql instance
            3. bucket_name: S3 bucket name
            4. schema_folder: folder path in S3 bucket
  */
  private void transfer2CSV(AmazonS3Client s3client, SQLContext sc, String schema_folder, String localStorage) {

    // get date folder. folder is named by datetime, like 20160402_20171029_20171102T170615.937Z
    ListObjectsRequest listFolders = new ListObjectsRequest()
        .withBucketName(bucket_name).withPrefix(schema_folder + "/").withDelimiter("/");
    ObjectListing foldersListing;
    do {
      foldersListing = s3client.listObjects(listFolders);
      for (String folder_path : foldersListing.getCommonPrefixes()) {
          // String[] split_str = folder_path.split("/");
          // String folder_name = split_str[split_str.length - 1];
          String target_folder = localStorage + "/" + folder_path;
          System.out.println("folder path is " + folder_path);

          // collect all parquet file paths
          List<String> s3Paths = new ArrayList<String>();

          // get all parquet files in the date folder and add to s3Paths
          ListObjectsRequest listObjectRequest = new ListObjectsRequest()
              .withBucketName(bucket_name).withPrefix(folder_path).withDelimiter("/");
          ObjectListing objectListing;
          do {
            objectListing = s3client.listObjects(listObjectRequest);
            for (S3ObjectSummary s3Object : objectListing.getObjectSummaries()) {
              if (s3Object.getKey().endsWith(".parquet")) {
                String absoluteS3Path = bucket_name + S3FileSeparator + s3Object.getKey();
                s3Paths.add(S3Prefix + absoluteS3Path);
                System.out.println(absoluteS3Path);
              }
            }
            // continue geting parquet files from last fetching marker
            listObjectRequest.setMarker(objectListing.getNextMarker());
          } while (objectListing.isTruncated() == true);

          // parallel read those files
          ListIterator<String> onefile = null;
	  onefile=s3Paths.listIterator();
	  while(onefile.hasNext()){
          	Dataset<Row> parquet_data = sc.read().parquet((onefile.next()));
          //	System.out.println("xxxxxxxxxxxxxxxxx schema for " + schema_folder + " xxxxxxxxxxxxxxxxx");
          //	parquet_data.printSchema();
         //	System.out.println("xxxxxxxxxxxxxxxxx schema for " + schema_folder + " xxxxxxxxxxxxxxxxx");
          // write the data to csv
          // parquet_data.write().csv(target_folder);
          	processHiveData(parquet_data, "", 0.2f, 1);
	  }
      }
      // continue geting date folder name from last fetching marker
      listFolders.setMarker(foldersListing.getNextMarker());
    } while(foldersListing.isTruncated() == true);

    // JavaRDD<String> rdd = context.parallelize(s3Paths);
    // JavaRDD<String> ops = rdd.map(filePath -> {
    //   Dataset<Row> onefile = new SQLContext(context).read().parquet(S3Prefix + filePath);
    //   // //processHiveData(onefile, "", 0.2f, 1);
    //   onefile.show();
    //   return "done";
    // });
    // String total = ops.reduce((a, b) -> a + b);
  }


  private void processHiveData(final Dataset<Row> hiveData, String hosts,
                             float validatePercentage, int wps) {

    hiveData.foreachPartition(rowIterator -> {
      int rowCount = 0;
      StringBuilder payload = new StringBuilder("");
      while (rowIterator.hasNext()) {
        Row row = rowIterator.next();
        rowCount++;
        for (int i=0; i<row.length()-1; i++){
            //System.out.println(i);
            //System.out.println(row.get(i));
            payload.append(Objects.toString(row.get(i), "") + ",");
        }
        payload.append(Objects.toString(row.get(row.length() -1),"") + "\n");

        if (rowCount == 5000){
          batchPost(payload, rowCount);
          rowCount = 0;
        }
      }
      //handle case for the last batch
       if (rowCount > 0){
          batchPost(payload, rowCount);
        }
    });
  }

  public void batchPost(StringBuilder payload, int count) {
    String endpoint = "";
    List<String> hosts = Arrays.asList("172.31.30.148");
    Random random = new Random();
    String host = hosts.get(random.nextInt(hosts.size()));
    // String requestUrl = "http://" + host + ":9000/dumpfile?filename=/tmp/data.csv";
    String requestUrl = "http://" + host + ":9000/ddl?sep=,&tag=loadClient&eol=%0A";
    String ret_val = sendPostRequest(requestUrl, payload.toString());
    payload.setLength(0);
  }

  public static String sendPostRequest(String requestUrl, String payload) {
    StringBuffer sb = new StringBuffer();
    try {
      URL url = new URL(requestUrl);
      HttpURLConnection connection = (HttpURLConnection) url.openConnection();

      connection.setDoInput(true);
      connection.setDoOutput(true);
      connection.setRequestMethod("POST");
      connection.setRequestProperty("Accept", "application/json");
      connection.setRequestProperty("Content-Type", "application/json; charset = UTF-8");
      OutputStreamWriter writer = new OutputStreamWriter(connection.getOutputStream(), "UTF-8");
      writer.write(payload);
      writer.close();
      BufferedReader br = new BufferedReader(new InputStreamReader(connection.getInputStream()));
      String line = null;
      while ((line = br.readLine()) != null) {
        sb.append(line);
      }
      br.close();
    } catch (Exception e) {
      throw new RuntimeException(e.getMessage());
    }
    return sb.toString();
  }
}
