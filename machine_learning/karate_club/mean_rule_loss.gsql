USE GRAPH karate_club
DROP QUERY mean_rule_loss

CREATE QUERY mean_rule_loss(/* Parameters here */) FOR GRAPH karate_club { 
/* Propagate: 
   1. Add self-loop
   2. normalization
	 3. weight
	 4. activation function
	 
	 Semi-Supervised Classification:
	 1. Perform forward propagation through the GCN.
   2. Apply the sigmoid function row-wise on the last layer in the GCN.
   3. Compute the cross entropy loss on known node labels.
   4. Backpropagate the loss and update the weight matrices W in each layer.
*/
	MapAccum<INT, DOUBLE> @features, @old_features, @z1, @z2;
	ListAccum<ListAccum<DOUBLE>> @@W_1, @@W_2;
	SumAccum<DOUBLE> @@loss; 
	INT n, n_layer1, n_layer2;
	
	// initialize weight
	@@W_1 += [[0.07958724,0.41393423,2.61319453,0.3974282]
,[1.7433738,-0.42154567,1.9645318,-1.1894275]
,[1.33326457,-0.11742257,-0.76402826,1.80756004]
,[-0.93287385,-1.03186914,-1.20829323,2.83467797]
,[-1.93607551,-0.34255435,0.16058544,1.44502767]
,[-0.83975565,1.26265919,-0.29115232,0.39024524]
,[0.68746817,1.12589213,0.48983897,-0.2819816]
,[0.20503101,-0.21094416,0.56781114,-1.29460156]
,[0.82850177,1.08452738,-0.40054922,-0.38113591]
,[-0.32132121,-0.16208315,0.09393929,0.15692015]
,[-1.84817319,1.15287529,0.35042802,1.29060893]
,[-0.10513013,0.99319201,1.10720677,0.17733811]
,[-0.33123951,-0.78584322,-1.00220281,-0.24719436]
,[-0.74898195,0.43883237,-0.3224015,-1.15195773]
,[0.71997328,-1.02099508,1.41645986,-0.22661059]
,[1.86900896,0.20020973,-0.11206868,0.29238163]
,[0.3336336,1.16015907,1.27842678,-0.48621194]
,[-1.31631368,-0.54264572,1.47825611,-0.04091017]
,[-0.06242896,-0.26758257,0.0072831,0.01817562]
,[0.02840864,-1.69125836,-0.01391545,-1.13737052]
,[-1.07452197,-0.90577322,-0.88108005,0.72557385]
,[-1.44646041,-1.7530518,0.32618876,-0.64484053]
,[0.47558205,-1.2365559,-2.20544062,-1.13161411]
,[0.30359955,1.73437499,1.09163664,0.53105984]
,[-0.50785298,0.22205533,-0.50664548,1.40717628]
,[-0.20354942,-0.1688182,-0.43944922,1.44116952]
,[-2.61969327,-0.78730932,-0.15409387,-0.80296547]
,[0.095067,-0.16837898,-1.20080908,-0.96101499]
,[0.12038993,1.2140307,-1.08564918,0.28648878]
,[-1.71487346,0.36095012,0.67088934,-0.93817124]
,[1.10054354,-2.97042878,-1.26233138,1.35631573]
,[0.8235679,-1.77120161,-0.10460913,1.26118244]
,[-0.21731931,-1.87528341,0.65372554,0.01150124]
,[-0.21227417,0.00803616,-0.04198122,0.48204424]];
	
	@@W_2 += [[0.38030149,0.97287903]
,[0.80663453,0.58020131]
,[1.06390283,0.37900075]
,[-0.05380065,1.29779552]];
	
	
	all_nodes = {Member.*};
	n = all_nodes.size();
	n_layer1 = @@W_1.get(0).size();
	n_layer2 = @@W_2.get(0).size();
	//PRINT @@W_1, @@W_2;
	//PRINT n_layer1, n_layer2;
	
	// initialize feature
	all_nodes = SELECT s
	            FROM all_nodes:s
	            POST-ACCUM FOREACH i IN RANGE[0, n-1] DO    
	                         s.@old_features += (i -> 0)
	                       END,
	                       s.@old_features += (s.member_id -> 1);
	//PRINT nodes;
	
	// first layer: tanh function
	nodes = SELECT t
	        FROM all_nodes:s -(:e) -> :t 
	        ACCUM t.@features += s.@old_features
	        POST-ACCUM INT t_outdegree = t.outdegree() + 1,
	                   DOUBLE tmp = 0,
	                   t.@features += t.@old_features,
	                   t.@old_features.clear(),
	                   FOREACH i IN RANGE[0, n_layer1-1] DO
	                       tmp = 0,
	                       FOREACH (key, value) IN t.@features DO
	                           tmp = tmp + value * @@W_1.get(key).get(i)/t_outdegree   // need to devide by outdegree
	                       END,
	                       // tanh activation function
                         t.@old_features += (i -> tanh(tmp))
	                   END,
	                   t.@features.clear()
	                   ;  
	PRINT nodes;
	
	// second layer: identity function
	nodes = SELECT t
	        FROM all_nodes:s -(:e) -> :t 
	        ACCUM t.@features += s.@old_features
	        POST-ACCUM INT t_outdegree = t.outdegree() + 1,
	                   DOUBLE tmp = 0,
	                   t.@features += t.@old_features,
	                   t.@old_features.clear(),
	                   FOREACH i IN RANGE[0, n_layer2-1] DO
	                       tmp = 0,
	                       FOREACH (key, value) IN t.@features DO
	                           tmp = tmp + value * @@W_2.get(key).get(i)/t_outdegree   // need to devide by outdegree
	                       END,
	                       // identity activation function
                         t.@old_features += (i -> tmp)
	                   END,
	                   t.@features.clear()
	                   ;  
	
	// apply softmax activation function (sigmoid works too here, but softmax is better)
	all_nodes = SELECT s
	               FROM all_nodes:s
	               POST-ACCUM double t0=0, double t1=0,
	                          t0 = exp(s.@old_features.get(0)),
	                          t1 = exp(s.@old_features.get(1)),
	                          s.@features += (0 -> t0/(t0+t1)),
	                          s.@features += (1 -> t1/(t0+t1));
	                          //FOREACH (key, value) IN s.@old_features DO
	                          //    s.@features += (key -> exp(value)/(exp(value)+1))
	                          //END;
	
	// calculate cross-entropy loss function              
	training_set = SELECT s
	               FROM all_nodes:s
	               WHERE s.in_training == TRUE
	               POST-ACCUM IF s.community_label == 0 THEN  // one-hot vector: [1 0]
	                              @@loss += -log(s.@features.get(0))   // -log(y^hat_0)
	                          ELSE
	                              @@loss += -log(s.@features.get(1))
	                          END;
	//@@loss = @@loss / training_set.size();
	PRINT @@loss;
	
	
 
}

