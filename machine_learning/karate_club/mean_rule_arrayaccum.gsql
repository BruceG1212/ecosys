USE GRAPH karate_club
DROP QUERY mean_rule_arrayaccum

CREATE QUERY mean_rule_arrayaccum(INT iter = 10, double learning_rate = 0.01, double epsilon = 0.000001) FOR GRAPH karate_club {
/* Propagate:
   1. Add self-loop
   2. normalization
         3. weight
         4. activation function

         Semi-Supervised Classification:
         1. Perform forward propagation through the GCN.
   2. Apply the sigmoid function row-wise on the last layer in the GCN.
   3. Compute the cross entropy loss on known node labels.
   4. Backpropagate the loss and update the weight matrices W in each layer.
*/
        //MapAccum<INT, DOUBLE> @features, @old_features, ;
        ArrayAccum<SumAccum<DOUBLE>> @x[34];
        ArrayAccum<SumAccum<DOUBLE>> @z1[4];
        ArrayAccum<SumAccum<DOUBLE>> @a1[4];
        ArrayAccum<SumAccum<DOUBLE>> @dz1[4];
        ArrayAccum<SumAccum<DOUBLE>> @z2[2];
        ArrayAccum<SumAccum<DOUBLE>> @a2[2];
        ArrayAccum<SumAccum<DOUBLE>> @dz2[2];
        ArrayAccum<SumAccum<DOUBLE>> @tmp_n[34];
	      ArrayAccum<SumAccum<DOUBLE>> @tmp_n1[4];
	      ArrayAccum<SumAccum<DOUBLE>> @tmp_n2[2];
        ArrayAccum<SumAccum<DOUBLE>> @@dW2[4][2];
        ArrayAccum<SumAccum<DOUBLE>> @@dW1[34][4];
        ListAccum<ListAccum<DOUBLE>> @@W_1, @@W_2;
        SumAccum<DOUBLE> @@loss, @@diff;
        INT n, n_layer1, n_layer2;

        // initialize weight
        // W1 is 34 X 4
        @@W_1 += [[0.07958724,0.41393423,2.61319453,0.3974282]
,[1.7433738,-0.42154567,1.9645318,-1.1894275]
,[1.33326457,-0.11742257,-0.76402826,1.80756004]
,[-0.93287385,-1.03186914,-1.20829323,2.83467797]
,[-1.93607551,-0.34255435,0.16058544,1.44502767]
,[-0.83975565,1.26265919,-0.29115232,0.39024524]
,[0.68746817,1.12589213,0.48983897,-0.2819816]
,[0.20503101,-0.21094416,0.56781114,-1.29460156]
,[0.82850177,1.08452738,-0.40054922,-0.38113591]
,[-0.32132121,-0.16208315,0.09393929,0.15692015]
,[-1.84817319,1.15287529,0.35042802,1.29060893]
,[-0.10513013,0.99319201,1.10720677,0.17733811]
,[-0.33123951,-0.78584322,-1.00220281,-0.24719436]
,[-0.74898195,0.43883237,-0.3224015,-1.15195773]
,[0.71997328,-1.02099508,1.41645986,-0.22661059]
,[1.86900896,0.20020973,-0.11206868,0.29238163]
,[0.3336336,1.16015907,1.27842678,-0.48621194]
,[-1.31631368,-0.54264572,1.47825611,-0.04091017]
,[-0.06242896,-0.26758257,0.0072831,0.01817562]
,[0.02840864,-1.69125836,-0.01391545,-1.13737052]
,[-1.07452197,-0.90577322,-0.88108005,0.72557385]
,[-1.44646041,-1.7530518,0.32618876,-0.64484053]
,[0.47558205,-1.2365559,-2.20544062,-1.13161411]
,[0.30359955,1.73437499,1.09163664,0.53105984]
,[-0.50785298,0.22205533,-0.50664548,1.40717628]
,[-0.20354942,-0.1688182,-0.43944922,1.44116952]
,[-2.61969327,-0.78730932,-0.15409387,-0.80296547]
,[0.095067,-0.16837898,-1.20080908,-0.96101499]
,[0.12038993,1.2140307,-1.08564918,0.28648878]
,[-1.71487346,0.36095012,0.67088934,-0.93817124]
,[1.10054354,-2.97042878,-1.26233138,1.35631573]
,[0.8235679,-1.77120161,-0.10460913,1.26118244]
,[-0.21731931,-1.87528341,0.65372554,0.01150124]
,[-0.21227417,0.00803616,-0.04198122,0.48204424]];

        // W2 is 4 X 2
        @@W_2 += [[0.38030149,0.97287903]
,[0.80663453,0.58020131]
,[1.06390283,0.37900075]
,[-0.05380065,1.29779552]];


        all_nodes = {Member.*};
        n = all_nodes.size();
        n_layer1 = @@W_1.get(0).size();
        n_layer2 = @@W_2.get(0).size();

        // initialize feature: x is 1 * n
        all_nodes = SELECT s
                    FROM all_nodes:s
                    POST-ACCUM //s.@x.reallocate(n)
                                s.@x[s.member_id] += 1  // build one-hot vector
	                             ;

 //       WHILE abs(@@diff) > epsilon LIMIT iter DO
	      all_nodes = SELECT s
                    FROM all_nodes:s 
	                  ACCUM s.@tmp_n.reallocate(n)  // tmp:1*n
	                        , s.@tmp_n1.reallocate(n_layer1)  // tmp:1*n1
	                        ;
        // first layer: z1 is 1 * n_layer1, W_1 is n * n_layer1
        nodes = SELECT t
                FROM all_nodes:s -(:e) -> :t
                ACCUM t.@tmp_n += s.@x   // tmp:1*n, x:1*n
                POST-ACCUM t.@tmp_n += t.@x   // add self
                           , t.@z1.reallocate(n_layer1)  // z1:1*n1
                           , t.@z1 = multiply_ArrayAccum_ListOfList(t.@tmp_n, @@W_1)  // tmp:1*n, W1:n*n1, z1:1*n1
                           , t.@z1 = multiply_ArrayAccum_const(t.@z1, 1.0/(t.outdegree()+1))  // z1:1*n1
                           , t.@a1 += tanh_ArrayAccum(t.@z1)  // tanh activation function
                           ;

        // second layer: z2 is 1 * n_layer2, W_2 is n_layer1 * n_layer2
        nodes = SELECT t
                FROM all_nodes:s -(:e) -> :t
                ACCUM t.@tmp_n1 += s.@z1   // z1:1*n1
                POST-ACCUM t.@tmp_n1 += t.@z1  // add self
                           , t.@z2.reallocate(n_layer2)
                           , t.@z2 = multiply_ArrayAccum_ListOfList(t.@z1, @@W_2)
                           , t.@z2 = multiply_ArrayAccum_const(t.@z2, 1.0/(t.outdegree()+1))
                           ;

        // apply softmax activation function (sigmoid works too here, but softmax is better)
        all_nodes = SELECT s
                       FROM all_nodes:s
                       POST-ACCUM double t0=0
                                  , double t1=0
                                  , t0 = exp(s.@z2[0])
                                  , t1 = exp(s.@z2[1])
                                  , s.@a2.reallocate(n_layer2)
                                  , s.@a2[0] += t0/(t0+t1)
                                  , s.@a2[1] += t1/(t0+t1);

        PRINT all_nodes;

        
        // calculate cross-entropy loss function
        //@@dW2.reallocate(n_layer1, n_layer2);
        //@@dW1.reallocate(n, n_layer1);

        training_set = SELECT s
                       FROM all_nodes:s
                       WHERE s.in_training == TRUE
                       POST-ACCUM IF s.community_label == 0 THEN  // one-hot vector: [1 0]
                                      @@loss += -log(s.@a2[0])  // -log(y^hat_0)   a is just y^hat
                                      , s.@dz2[0] += s.@a2[0] - 1
                                      , s.@dz2[1] += s.@a2[1] - 0
                                  ELSE
                                      @@loss += -log(s.@a2[1])/n
                                  END
                                  // backpropagation
	                                , s.@tmp_n2.reallocate(n_layer2)
	                                , s.@tmp_n2 = multiply_ArrayAccum_const(s.@dz2, 1/n)
	                                , @@dW2 += multiply_ArrayAccum_ArrayAccum(s.@a1, s.@tmp_n2)
                                 
                                  // dz1 = W2^Tdz2 * g[1]'(z1)
                                  , s.@dz1 = multiply_ArrayAccum_ListOfListT(s.@dz2, @@W_2)
                                  , s.@tmp_n1.reallocate(n_layer1)
                                  , s.@tmp_n1 = tanh_prime_ArrayAccum(s.@z1)
                                  , s.@dz1 = elementProduct_ArrayAccum_ArrayAccum(s.@dz1, s.@tmp_n1)   // derivative of tanh is 1-tanh(z)^2
	                                , log(TRUE, "before", s.@tmp_n1)
	                                , s.@tmp_n1.reallocate(n_layer1)
	                                , log(TRUE, "after", s.@tmp_n1)
	                                , s.@tmp_n1 = multiply_ArrayAccum_const(s.@dz1, 1/n)
	                                , @@dW1 += multiply_ArrayAccum_ArrayAccum(s.@x, s.@tmp_n1)

                                  ;
	

  @@diff = @@diff - @@loss;
        PRINT @@loss;

 //       END;
        
}
